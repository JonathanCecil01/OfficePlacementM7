{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM8quG7AHIu7rFmg/13Tb+3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JonathanCecil01/OfficePlacementM7/blob/main/Haystack_Based_Context_Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ4Iwf5Mfbhl"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "pip install --upgrade pip\n",
        "pip install farm-haystack[colab,faiss,inference,ocr,preprocessing,file-conversion,pdf]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kQWEUUMnzqLX",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
        "logging.getLogger(\"haystack\").setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cYgDJmrA6Nv",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from haystack.document_stores import FAISSDocumentStore\n",
        "\n",
        "document_store = FAISSDocumentStore(faiss_index_factory_str=\"Flat\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.utils import clean_wiki_text, convert_files_to_docs, fetch_archive_from_http\n",
        "from haystack.nodes import PreProcessor\n",
        "\n",
        "# Let's first get some files that we want to use\n",
        "doc_dir = \"/content/data\"\n",
        "\n",
        "docs = convert_files_to_docs(dir_path=doc_dir, clean_func=clean_wiki_text, split_paragraphs=True)\n",
        "preprocessor = PreProcessor(\n",
        "    clean_empty_lines=True,\n",
        "    clean_whitespace=True,\n",
        "    clean_header_footer=False,\n",
        "    split_by=\"word\",\n",
        "    split_length=100,\n",
        "    split_respect_sentence_boundary=True,\n",
        ")\n",
        "preprocessed_docs = preprocessor.process(docs)\n",
        "print(f\"n_docs_input: 1\\nn_docs_output: {len(preprocessed_docs)}\")\n",
        "# Now, let's write the dicts containing documents to our DB.\n",
        "document_store.write_documents(preprocessed_docs)"
      ],
      "metadata": {
        "id": "_UiN8Uq4gRn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.nodes import EmbeddingRetriever\n",
        "\n",
        "retriever = EmbeddingRetriever(\n",
        "    document_store=document_store, embedding_model=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
        ")\n",
        "# Important:\n",
        "# Now that we initialized the Retriever, we need to call update_embeddings() to iterate over all\n",
        "# previously indexed documents and update their embedding representation.\n",
        "# While this can be a time consuming operation (depending on the corpus size), it only needs to be done once.\n",
        "# At query time, we only need to embed the query and compare it to the existing document embeddings, which is very fast.\n",
        "document_store.update_embeddings(retriever)"
      ],
      "metadata": {
        "id": "2zCc_WgTjZK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyIuWVwhA6OB"
      },
      "outputs": [],
      "source": [
        "from haystack.nodes import FARMReader\n",
        "\n",
        "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.pipelines import ExtractiveQAPipeline\n",
        "\n",
        "pipe = ExtractiveQAPipeline(reader, retriever)"
      ],
      "metadata": {
        "id": "Ussr764kjqVp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.utils import print_answers\n",
        "\n",
        "prediction = pipe.run(\n",
        "    query=\"How much is the Main Fund?\", params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}}\n",
        ")\n",
        "print(prediction['answers'][0].meta['name'])\n",
        "print_answers(prediction, details=\"minimum\")"
      ],
      "metadata": {
        "id": "g52UqQBVjrOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prediction['answers'][0].meta)"
      ],
      "metadata": {
        "id": "E2iwd6L-uWWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = [\"what is the Fund Name?\", \"When is the Start Date?\", \"Which Section has Carried Interest?\", \"Who is the General Partner?\", \"Which secition is about Initial Closing Date?\", \"When is the Final Closing Date?\", \"Which is the Management Company?\", \"What are the Investment Limitations?\", \"What is the Purpose?\", \"How long is the Partnership Term?\", \"How much is the Main Fund?\", \"How Much is the Transaction Fees?\", \"How much is the Makeup Contribution?\"]\n",
        "predictions = {}\n",
        "for query in queries:\n",
        "    prediction = pipe.run( query, params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}})\n",
        "    predictions[query] = []\n",
        "    for i in prediction['answers']:\n",
        "      temp_dict = {}\n",
        "      temp_dict['answer'] = i.answer\n",
        "      temp_dict['context'] = i.context\n",
        "      temp_dict['filename'] = i.meta['name']\n",
        "      temp_dict['score'] = i.score\n",
        "\n",
        "      predictions[query].append(temp_dict)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "2zv1e0IInlQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "json_string = json.dumps(predictions, indent = 2)\n",
        "with open(\"context_search_results.json\", \"w\") as f:\n",
        "  f.write(json_string)"
      ],
      "metadata": {
        "id": "leBbJANKnn9J"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}