{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLifub8mLfe1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters\n",
        "input_dim = 784  # Dimensionality of input data\n",
        "encoding_dim = 64  # Dimensionality of the code layer\n",
        "num_clusters = 10  # Number of clusters\n",
        "lambda_value = 0.1  # Hyperparameter controlling the trade-off between reconstruction and clustering\n"
      ],
      "metadata": {
        "id": "OlsiunrZM10K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the dataset\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "print (x_train.shape)\n",
        "print (x_test.shape)"
      ],
      "metadata": {
        "id": "G1NdbmzCM7tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize sample assignment randomly\n",
        "sample_assignments = np.random.randint(num_clusters, size=len(x_train))\n",
        "\n",
        "# Initialize cluster centers\n",
        "cluster_centers = tf.Variable(tf.random.normal(shape=(num_clusters, encoding_dim)))"
      ],
      "metadata": {
        "id": "koWeXwgcC_OM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 64\n",
        "\n",
        "class Autoencoder(Model):\n",
        "  def __init__(self, latent_dim):\n",
        "    super(Autoencoder, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "    self.encoder = tf.keras.Sequential([\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(latent_dim, activation='relu'),\n",
        "    ])\n",
        "    self.decoder = tf.keras.Sequential([\n",
        "      layers.Dense(784, activation='sigmoid'),\n",
        "      layers.Reshape((28, 28))\n",
        "    ])\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "\n",
        "\n",
        "autoencoder = Autoencoder(latent_dim)\n"
      ],
      "metadata": {
        "id": "ztF-7p7NzTKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clustering_loss(y_true, y_pred):\n",
        "  # Compute the reconstruction loss\n",
        "  reconstruction_loss = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
        "\n",
        "  # Compute the clustering loss\n",
        "  encoded_inputs = autoencoder.encoder(y_true)  # Get the encoded representations\n",
        "  expanded_centers = tf.expand_dims(cluster_centers, axis=0)\n",
        "  distances = tf.reduce_sum(tf.square(tf.expand_dims(encoded_inputs, axis=1) - expanded_centers), axis=2)\n",
        "  closest_cluster = tf.argmin(distances, axis=1)\n",
        "  clustering_loss = tf.reduce_mean(tf.reduce_sum(distances, axis=1))\n",
        "\n",
        "  # Combine the reconstruction loss and clustering loss\n",
        "  total_loss = reconstruction_loss - lambda_value * clustering_loss\n",
        "\n",
        "  return total_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "MaJEQ5pRBgbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the update equation for cluster centers\n",
        "def update_cluster_centers(X, sample_assignments):\n",
        "    cluster_centers = []\n",
        "    for j in range(num_clusters):\n",
        "        samples_in_cluster = X[sample_assignments == j]\n",
        "        cluster_center = tf.reduce_mean(samples_in_cluster, axis=0)\n",
        "        cluster_centers.append(cluster_center)\n",
        "    return cluster_centers"
      ],
      "metadata": {
        "id": "fjbmZw_xC1nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder = Autoencoder(latent_dim)\n",
        "autoencoder.compile(optimizer = 'adam', loss = clustering_loss)"
      ],
      "metadata": {
        "id": "XCitnulH0MPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "max_iterations = 200  # Maximum number of iterations\n",
        "x_train_reshaped = x_train.reshape(-1, 784)\n",
        "for t in range(max_iterations):\n",
        "    # Train the auto-encoder\n",
        "    autoencoder.fit(x_train, x_train, batch_size=6400, epochs=1)\n",
        "\n",
        "    # Update the cluster centers\n",
        "    cluster_centers.assign(update_cluster_centers(autoencoder.encoder.predict(x_train), sample_assignments))\n",
        "\n",
        "    # Update the sample assignment\n",
        "    encoded_samples = autoencoder.encoder(x_train_reshaped)\n",
        "\n",
        "    # Compute distances between encoded samples and cluster centers\n",
        "    distances = tf.reduce_sum(tf.square(tf.expand_dims(encoded_samples, axis=1) - cluster_centers), axis=2)\n",
        "\n",
        "    # Assign each sample to the closest cluster\n",
        "    sample_assignments = tf.argmin(distances, axis=1)\n",
        "\n",
        "\n",
        "# Final sample assignment\n",
        "final_sample_assignment = sample_assignments\n"
      ],
      "metadata": {
        "id": "ilfvKzR5NJPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_sample_assignment = sample_assignments"
      ],
      "metadata": {
        "id": "E6xPGai_YBfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "mnist = fetch_openml('mnist_784')\n",
        "\n",
        "labels = mnist.target\n",
        "labels = [int(i) for i in labels]"
      ],
      "metadata": {
        "id": "NriUpid7gaCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import adjusted_rand_score, silhouette_score"
      ],
      "metadata": {
        "id": "vXZJcQcwiAF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ari = adjusted_rand_score(labels[:60000], final_sample_assignment)"
      ],
      "metadata": {
        "id": "JMQOz5Nuh1kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels[0:10]"
      ],
      "metadata": {
        "id": "jq-zGBywiFgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_sample_assignment[0:10]"
      ],
      "metadata": {
        "id": "qhIBD0GAlzT7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}